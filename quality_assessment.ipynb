{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn off Windows smart screen\n",
    "# Turn off File security system\n",
    "# Change power options to make sure that the computer doesn't sleep\n",
    "# Connect the power cable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymysql\n",
    "import scipy.stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "logging.info('Logging starts')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Darts-ip database re-OCRing\n",
      "Enter the language:de\n",
      " Enter the number of files to be cleansed\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"Welcome to Darts-ip database re-OCRing\"\n",
    "print \"Enter the language:\",\n",
    "lang = raw_input()\n",
    "print \"Enter the number of files to be cleansed\"\n",
    "no_files=raw_input()\n",
    "query = \"SELECT textdata, filename, decision_fk FROM plaintext WHERE fulltextdarts.plaintext.language='\"+str(lang)+ \"' LIMIT \"  +str(no_files)+\";\"\n",
    "\n",
    "logging.info(\"The SQL query executed is:\")\n",
    "logging.info(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "f=open(r\"C:\\Users\\vbask\\Desktop\\start_time.txt\",\"wb\")\n",
    "f.write(str(datetime.datetime.now()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info('Training the Quality Detection algorithm')\n",
    "german_train_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\English_Decisions_Distribution\\\\German Training files\"\n",
    "\n",
    "os.chdir(german_train_path)\n",
    "\n",
    "X_array=pickle.load(open(\"X_array.p\",\"rb\"))\n",
    "training_poor_fk=pickle.load(open(\"training_poor_fk.p\",\"rb\"))\n",
    "training_poor_filename=pickle.load(open(\"training_poor_filename.p\",\"rb\"))\n",
    "training_poor_text=pickle.load(open(\"training_poor_text.p\",\"rb\"))\n",
    "training_poor_text_processed=pickle.load(open(\"training_poor_text_processed.p\",\"rb\"))\n",
    "X_array_training_poor=pickle.load(open(\"X_array_training_poor.p\",\"rb\"))\n",
    "\n",
    "training_wrong_lang_fk=pickle.load(open(\"training_wrong_lang_fk.p\",\"rb\"))\n",
    "training_wrong_lang_filename=pickle.load(open(\"training_wrong_lang_filename.p\",\"rb\"))\n",
    "training_wrong_lang_text=pickle.load(open(\"training_wrong_lang_text.p\",\"rb\"))\n",
    "training_wrong_lang_text_processed=pickle.load(open(\"training_wrong_lang_text_processed.p\",\"rb\"))\n",
    "X_array_training_wrong_lang=pickle.load(open(\"X_array_training_wrong_lang.p\",\"rb\"))\n",
    "\n",
    "training_good_fk=pickle.load(open(\"training_good_fk.p\",\"rb\"))\n",
    "training_good_filename=pickle.load(open(\"training_good_filename.p\",\"rb\"))\n",
    "training_good_text=pickle.load(open(\"training_good_text.p\",\"rb\"))\n",
    "training_good_text_processed=pickle.load(open(\"training_good_text_processed.p\",\"rb\"))\n",
    "X_array_training_good=pickle.load(open(\"X_array_training_good.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(X_array_training_good[:,1],X_array_training_good[:,0],'go',label=\"Good\")\n",
    "# plt.plot(X_array_training_wrong_lang[:,1],X_array_training_wrong_lang[:,0],'yo',label=\"Wrong language\")\n",
    "# plt.plot(X_array_training_poor[:,1],X_array_training_poor[:,0],'ro',label=\"Poor\")\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Length of tokens')\n",
    "# plt.ylabel('% of tokens matching a dictionary')\n",
    "# plt.title('Training files visualization')\n",
    "# plt.xlim(0,4000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array_training=np.vstack((X_array_training_poor,X_array_training_wrong_lang,X_array_training_good))\n",
    "Y_array_training=[0 for i in range(len(X_array_training_poor))]+[0 for i in range(len(X_array_training_wrong_lang))]+[2 for i in range(len(X_array_training_good))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight={0: 7, 2: 1}, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "scaler=preprocessing.StandardScaler().fit(X_array_training)\n",
    "X_array_training_scaled=scaler.transform(X_array_training)\n",
    "\n",
    "#clf = svm.SVC(class_weight={0:7,2:1},kernel='rbf',gamma=3)\n",
    "clf = svm.SVC(class_weight={0:7,2:1},kernel='linear')\n",
    "clf.fit(X_array_training_scaled,Y_array_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create a mesh to plot in\n",
    "# h=0.005\n",
    "# x_min, x_max = X_array_training_scaled[:, 0].min()-1, X_array_training_scaled[:, 0].max()+1\n",
    "# y_min, y_max = X_array_training_scaled[:, 1].min()-1, X_array_training_scaled[:, 1].max()+1\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# inp=np.c_[xx.ravel(), yy.ravel()]\n",
    "# inp_pred=clf.predict(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inp_pred=clf.predict(inp)\n",
    "# plt.plot(inp[inp_pred==2][:,1],inp[inp_pred==2][:,0],'g.')\n",
    "# plt.plot(inp[inp_pred==0][:,1],inp[inp_pred==0][:,0],'r.')\n",
    "# plt.plot(X_array_training_scaled[np.asarray(Y_array_training)==2][:,1],X_array_training_scaled[np.asarray(Y_array_training)==2][:,0],'go')\n",
    "# plt.plot(X_array_training_scaled[np.asarray(Y_array_training)==0][:,1],X_array_training_scaled[np.asarray(Y_array_training)==0][:,0],'ro')\n",
    "# plt.xlabel('Normalized length of tokens')\n",
    "# plt.ylabel('Normalized % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear vs Non-linear SVM')\n",
    "# plt.xlim(-0.3,0.3)\n",
    "# plt.ylim(-1.5,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info('Training completed')\n",
    "logging.info('Connecting to the SQL Server')\n",
    "try:\n",
    "    db = pymysql.connect(host=\"localhost\", port=3320, user=\"ops\", passwd=\"target\", db=\"fulltextdarts\", charset=\"utf8\")\n",
    "    cursor = db.cursor()\n",
    "except:\n",
    "    logging.error(\"Unable to connect the SQL server\")\n",
    "    db = pymysql.connect(host=\"localhost\", port=3320, user=\"ops\", passwd=\"target\", db=\"fulltextdarts\", charset=\"utf8\")\n",
    "    cursor = db.cursor()\n",
    "\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "tbl = dict.fromkeys(i for i in xrange(sys.maxunicode)\n",
    "                      if unicodedata.category(unichr(i)).startswith('P'))\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def extract_quality_features(text,dictionary_words,show=1):\n",
    "    percent_tokens=[]\n",
    "    len_tokens=[]\n",
    "    text_processed=[]\n",
    "    for i in np.arange(len(text)):\n",
    "        temp=text[i].replace(u\"\\u2018\", \"\").replace(u\"\\u2019\", \"\").replace(u\"\\n\",\" \")\n",
    "        tokens=temp.split()\n",
    "        tokens=[remove_punctuation(word.lower()) for word in tokens]\n",
    "        punctuations = re.compile(r'[-./?!,--&\":;()|0-9]')\n",
    "        tokens=[punctuations.sub(\"\", word) for word in tokens if punctuations.sub(\"\", word)]\n",
    "        text_processed.append(\" \".join(tokens))\n",
    "        len_tokens.append(len(tokens))   \n",
    "\n",
    "    vect=CountVectorizer(vocabulary=dictionary_words)\n",
    "    X_counts=vect.transform(text_processed)\n",
    "    X_counts_sum=X_counts.sum(axis=1)\n",
    "    X_counts_sum = np.squeeze(np.asarray(X_counts_sum))\n",
    "\n",
    "    percent_tokens=[]\n",
    "    for i in range(len(text)):\n",
    "        temp=X_counts_sum[i]/len_tokens[i]\n",
    "        if np.isfinite(temp):\n",
    "            percent_tokens.append(temp if temp<1 else 1)\n",
    "        else:\n",
    "            percent_tokens.append(0)\n",
    "    if show:\n",
    "        print \"Do nothing\"\n",
    "        #plt.plot(len_tokens,percent_tokens,'.')\n",
    "        #plt.ylim(0,1)\n",
    "        #plt.xlim(0,4000)\n",
    "        #plt.xlabel('Length of the Document')\n",
    "        #plt.ylabel('% of words matching dictionary')\n",
    "        #plt.title(str(len(percent_tokens))+' documents features plot')\n",
    "        #plt.show()\n",
    "    return percent_tokens,len_tokens,text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\vbask\\Documents\\Darts_IP\\Darts_IP\\Dictionaries')\n",
    "logging.info('Loading Latin Russian Dictionary')\n",
    "dictionary_words=pickle.load(open(\"latin_rus_dic.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#query =\"SELECT textdata, filename, decision_fk FROM plaintext WHERE fulltextdarts.plaintext.language='de' LIMIT 1;\"\n",
    "# db = pymysql.connect(host=\"localhost\", port=3320, user=\"ops\", passwd=\"target\", db=\"fulltextdarts\", charset=\"utf8\")\n",
    "# cursor = db.cursor()\n",
    "logging.info('Executing SQL Query')\n",
    "cursor.execute(query)\n",
    "\n",
    "test_text=[];test_text_filename=[];test_text_fk=[]\n",
    "for row in cursor:\n",
    "    test_text.append(row[0])\n",
    "    test_text_filename.append(row[1])\n",
    "    test_text_fk.append(row[2])\n",
    "if len(test_text)==1:\n",
    "    test_text.append('')\n",
    "    test_text_filename.append('')\n",
    "    test_text_fk.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do nothing\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Extracting features from the test files\")\n",
    "test_percent_tokens,test_len_tokens,test_text_processed=extract_quality_features(test_text,dictionary_words)\n",
    "X_array_test=np.vstack((np.asarray(test_percent_tokens),np.asarray(test_len_tokens))).T\n",
    "X_array_test_scaled=scaler.transform(X_array_test)\n",
    "logging.info(\"Successfully extracted features from the test file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info(\"Predicting the quality of the test files\")\n",
    "clf = svm.SVC(class_weight={0:7,2:1},kernel='linear')\n",
    "clf.fit(X_array_training_scaled,Y_array_training)\n",
    "X_array_test_pred_7=clf.predict(X_array_test_scaled)\n",
    "X_array_test_pred=X_array_test_pred_7\n",
    "\n",
    "# plt.plot(X_array_test[X_array_test_pred==2][:,1],X_array_test[X_array_test_pred==2][:,0],'g.')\n",
    "# plt.plot(X_array_test[X_array_test_pred==0][:,1],X_array_test[X_array_test_pred==0][:,0],'r.')\n",
    "\n",
    "# plt.xlabel('Test length of tokens')\n",
    "# plt.ylabel('Test % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear SVM for test data')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_array_test_scaled[X_array_test_pred==2][:,1],X_array_test_scaled[X_array_test_pred==2][:,0],'g.')\n",
    "# plt.plot(X_array_test_scaled[X_array_test_pred==0][:,1],X_array_test_scaled[X_array_test_pred==0][:,0],'r.')\n",
    "\n",
    "# plt.xlabel('Normalized test length of tokens')\n",
    "# plt.ylabel('Normalized test % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear SVM for normalized test data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(class_weight={0:1,2:1},kernel='linear')\n",
    "clf.fit(X_array_training_scaled,Y_array_training)\n",
    "\n",
    "X_array_test_pred_1=clf.predict(X_array_test_scaled)\n",
    "X_array_test_pred=X_array_test_pred_1\n",
    "# plt.plot(X_array_test[X_array_test_pred==2][:,1],X_array_test[X_array_test_pred==2][:,0],'g.')\n",
    "# plt.plot(X_array_test[X_array_test_pred==0][:,1],X_array_test[X_array_test_pred==0][:,0],'r.')\n",
    "\n",
    "# plt.xlabel('Test length of tokens')\n",
    "# plt.ylabel('Test % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear SVM for test data')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_array_test_scaled[X_array_test_pred==2][:,1],X_array_test_scaled[X_array_test_pred==2][:,0],'g.')\n",
    "# plt.plot(X_array_test_scaled[X_array_test_pred==0][:,1],X_array_test_scaled[X_array_test_pred==0][:,0],'r.')\n",
    "\n",
    "# plt.xlabel('Normalized test length of tokens')\n",
    "# plt.ylabel('Normalized test % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear SVM for normalized test data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_prior_filename=[fname for fname in list(np.asarray(test_text_filename)[X_array_test_pred_1!=X_array_test_pred_7])+list(np.asarray(test_text_filename)[np.asarray(test_len_tokens)<10])]\n",
    "batch_non_prior_filename=list(set([fname for fname in list(np.asarray(test_text_filename)[X_array_test_pred_1==0])])-set(batch_prior_filename))\n",
    "\n",
    "# print(\"Batch Prior\")\n",
    "# for fname in batch_prior_filename:\n",
    "#     print(str(os.path.splitext(fname)[0])+\",\")\n",
    "# print(\"Batch Non Prior\")\n",
    "# for fname in batch_non_prior_filename:\n",
    "#     print(str(os.path.splitext(fname)[0])+\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PDF_files_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\PDF_Files\\\\DE_30000\"\n",
    "PDF_files_names=[f for f in os.listdir(PDF_files_path) if os.path.splitext(f)[1]==\".pdf\"]\n",
    "downloaded_prior_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\Batch_prior\"\n",
    "downloaded_non_prior_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\Batch_non_prior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info(\"Deleting previous files in download prior and non-prior paths\")\n",
    "for root, dirs, files in os.walk(downloaded_prior_path):\n",
    "    for f in files:\n",
    "        os.unlink(os.path.join(root, f))\n",
    "for root, dirs, files in os.walk(downloaded_non_prior_path):\n",
    "    for f in files:\n",
    "        os.unlink(os.path.join(root, f))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logging.info(\"Moving files based on priority\")\n",
    "for fname in batch_prior_filename:\n",
    "    if fname in PDF_files_names:\n",
    "        shutil.copy(PDF_files_path+\"\\\\\"+fname,downloaded_prior_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\")\n",
    "pickle.dump(X_array_test,open(\"X_array_test.p\",\"wb\"))\n",
    "pickle.dump(test_text_filename,open(\"test_text_filename.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fname in batch_non_prior_filename:\n",
    "    if fname in PDF_files_names:\n",
    "        shutil.copy(PDF_files_path+\"\\\\\"+fname,downloaded_non_prior_path)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
