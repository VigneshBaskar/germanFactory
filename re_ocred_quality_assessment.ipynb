{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymysql\n",
    "import scipy.stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_train_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\English_Decisions_Distribution\\\\German Training files\"\n",
    "\n",
    "os.chdir(german_train_path)\n",
    "\n",
    "X_array=pickle.load(open(\"X_array.p\",\"rb\"))\n",
    "training_poor_fk=pickle.load(open(\"training_poor_fk.p\",\"rb\"))\n",
    "training_poor_filename=pickle.load(open(\"training_poor_filename.p\",\"rb\"))\n",
    "training_poor_text=pickle.load(open(\"training_poor_text.p\",\"rb\"))\n",
    "training_poor_text_processed=pickle.load(open(\"training_poor_text_processed.p\",\"rb\"))\n",
    "X_array_training_poor=pickle.load(open(\"X_array_training_poor.p\",\"rb\"))\n",
    "\n",
    "training_wrong_lang_fk=pickle.load(open(\"training_wrong_lang_fk.p\",\"rb\"))\n",
    "training_wrong_lang_filename=pickle.load(open(\"training_wrong_lang_filename.p\",\"rb\"))\n",
    "training_wrong_lang_text=pickle.load(open(\"training_wrong_lang_text.p\",\"rb\"))\n",
    "training_wrong_lang_text_processed=pickle.load(open(\"training_wrong_lang_text_processed.p\",\"rb\"))\n",
    "X_array_training_wrong_lang=pickle.load(open(\"X_array_training_wrong_lang.p\",\"rb\"))\n",
    "\n",
    "training_good_fk=pickle.load(open(\"training_good_fk.p\",\"rb\"))\n",
    "training_good_filename=pickle.load(open(\"training_good_filename.p\",\"rb\"))\n",
    "training_good_text=pickle.load(open(\"training_good_text.p\",\"rb\"))\n",
    "training_good_text_processed=pickle.load(open(\"training_good_text_processed.p\",\"rb\"))\n",
    "X_array_training_good=pickle.load(open(\"X_array_training_good.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(X_array_training_good[:,1],X_array_training_good[:,0],'go',label=\"Good\")\n",
    "# plt.plot(X_array_training_wrong_lang[:,1],X_array_training_wrong_lang[:,0],'yo',label=\"Wrong language\")\n",
    "# plt.plot(X_array_training_poor[:,1],X_array_training_poor[:,0],'ro',label=\"Poor\")\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Length of tokens')\n",
    "# plt.ylabel('% of tokens matching a dictionary')\n",
    "# plt.title('Training files visualization')\n",
    "# plt.xlim(0,4000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array_training=np.vstack((X_array_training_poor,X_array_training_wrong_lang,X_array_training_good))\n",
    "Y_array_training=[0 for i in range(len(X_array_training_poor))]+[0 for i in range(len(X_array_training_wrong_lang))]+[2 for i in range(len(X_array_training_good))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight={0: 7, 2: 1}, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "scaler=preprocessing.StandardScaler().fit(X_array_training)\n",
    "X_array_training_scaled=scaler.transform(X_array_training)\n",
    "\n",
    "#clf = svm.SVC(class_weight={0:7,2:1},kernel='rbf',gamma=3)\n",
    "clf = svm.SVC(class_weight={0:7,2:1},kernel='linear')\n",
    "clf.fit(X_array_training_scaled,Y_array_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pareums=set()\n",
    "    else:\n",
    "        pareums=set(pages)\n",
    "        \n",
    "    output=io.BytesIO()\n",
    "    manager=PDFResourceManager()\n",
    "    converter=TextConverter(manager, output, codec='utf-8', laparams=LAParams())\n",
    "    interpreter=PDFPageInterpreter(manager,converter)\n",
    "    \n",
    "    infile=file(fname,'rb')\n",
    "    for page in PDFPage.get_pages(infile, pareums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text=output.getvalue()\n",
    "    output.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "tbl = dict.fromkeys(i for i in xrange(sys.maxunicode)\n",
    "                      if unicodedata.category(unichr(i)).startswith('P'))\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def extract_quality_features(text,dictionary_words,show=1):\n",
    "    percent_tokens=[]\n",
    "    len_tokens=[]\n",
    "    text_processed=[]\n",
    "    for i in np.arange(len(text)):\n",
    "        temp=text[i].replace(u\"\\u2018\", \"\").replace(u\"\\u2019\", \"\").replace(u\"\\n\",\" \")\n",
    "        tokens=temp.split()\n",
    "        tokens=[remove_punctuation(word.lower()) for word in tokens]\n",
    "        punctuations = re.compile(r'[-./?!,--&\":;()|0-9]')\n",
    "        tokens=[punctuations.sub(\"\", word) for word in tokens if punctuations.sub(\"\", word)]\n",
    "        text_processed.append(\" \".join(tokens))\n",
    "        len_tokens.append(len(tokens))   \n",
    "\n",
    "    vect=CountVectorizer(vocabulary=dictionary_words)\n",
    "    X_counts=vect.transform(text_processed)\n",
    "    X_counts_sum=X_counts.sum(axis=1)\n",
    "    X_counts_sum = np.squeeze(np.asarray(X_counts_sum))\n",
    "\n",
    "    percent_tokens=[]\n",
    "    for i in range(len(text)):\n",
    "        temp=X_counts_sum[i]/len_tokens[i]\n",
    "        if np.isfinite(temp):\n",
    "            percent_tokens.append(temp if temp<1 else 1)\n",
    "        else:\n",
    "            percent_tokens.append(0)\n",
    "    if show:\n",
    "        print \"Do nothing\"\n",
    "    return percent_tokens,len_tokens,text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\vbask\\Documents\\Darts_IP\\Darts_IP\\Dictionaries')\n",
    "dictionary_words=pickle.load(open(\"latin_rus_dic.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darts-226-398-A-de.pdf', 'darts-259-916-A-de.pdf', 'darts-935-617-C-de.pdf']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "re_OCRed_text_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\4_b_Re_OCRed\"\n",
    "os.chdir(re_OCRed_text_path)\n",
    "\n",
    "re_OCRed_text_filename=[f for f in os.listdir(re_OCRed_text_path) if os.path.isfile(f) and os.path.splitext(f)[1]==\".pdf\"]\n",
    "print re_OCRed_text_filename\n",
    "print len(re_OCRed_text_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success darts-226-398-A-de.pdf\n",
      "Success darts-259-916-A-de.pdf\n",
      "Success darts-935-617-C-de.pdf\n",
      "\n",
      "\n",
      "2.82500004768\n",
      "Non OCR Prediciton through error\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1=time.time()\n",
    "import os\n",
    "\n",
    "os.chdir(re_OCRed_text_path)\n",
    "re_OCRed_text=[]\n",
    "erroneous_fnames=[]\n",
    "non_erroneous_fnames=[]\n",
    "\n",
    "for fname in re_OCRed_text_filename:\n",
    "    try:\n",
    "        print(\"Success \"+fname)\n",
    "        re_OCRed_text.append(convert(fname).decode('utf-8','ignore'))\n",
    "        non_erroneous_fnames.append(fname)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        print('Some Error skipped with try')\n",
    "        print fname\n",
    "        erroneous_fnames.append(fname)\n",
    "\n",
    "time2=time.time()\n",
    "print('\\n')\n",
    "print (time2-time1)\n",
    "print('Non OCR Prediciton through error')\n",
    "print erroneous_fnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_OCRed_text_filename=non_erroneous_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re_OCRed_text_percent_tokens,re_OCRed_text_len_tokens,re_OCRed_text_processed=extract_quality_features(re_OCRed_text,dictionary_words,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array_re_ocred=np.vstack((np.asarray(re_OCRed_text_percent_tokens),np.asarray(re_OCRed_text_len_tokens))).T\n",
    "X_array_re_ocred_scaled=scaler.transform(X_array_re_ocred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(class_weight={0:7,2:1},kernel='rbf',probability=True)\n",
    "clf.fit(X_array_training_scaled,Y_array_training)\n",
    "X_array_re_ocred_pred_7=clf.predict(X_array_re_ocred_scaled)\n",
    "X_array_re_ocred_pred=X_array_re_ocred_pred_7\n",
    "\n",
    "# plt.plot(X_array_re_ocred[X_array_re_ocred_pred==2][:,1],X_array_re_ocred[X_array_re_ocred_pred==2][:,0],'g.')\n",
    "# plt.plot(X_array_re_ocred[X_array_re_ocred_pred==0][:,1],X_array_re_ocred[X_array_re_ocred_pred==0][:,0],'r.')\n",
    "\n",
    "# plt.xlabel('Test length of tokens')\n",
    "# plt.ylabel('Test % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear SVM for test data')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_array_re_ocred_scaled[X_array_re_ocred_pred==2][:,1],X_array_re_ocred_scaled[X_array_re_ocred_pred==2][:,0],'g.')\n",
    "# plt.plot(X_array_re_ocred_scaled[X_array_re_ocred_pred==0][:,1],X_array_re_ocred_scaled[X_array_re_ocred_pred==0][:,0],'r.')\n",
    "\n",
    "# plt.xlabel('Normalized test length of tokens')\n",
    "# plt.ylabel('Normalized test % of tokens matching a dictionary')\n",
    "# plt.title('Decision boundary of Linear SVM for normalized test data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array_test=pickle.load(open(\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\X_array_test.p\",\"rb\"))\n",
    "test_text_filename=pickle.load(open(\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\test_text_filename.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_array_test_scaled=scaler.transform(X_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "improvements=[clf.predict_proba(X_array_re_ocred_scaled[re_OCRed_text_filename.index(re_OCRed_text_filename[index])])[0][1] -clf.predict_proba(X_array_test_scaled[test_text_filename.index(re_OCRed_text_filename[index])])[0][1] for index in range(len(re_OCRed_text_filename))]\n",
    "X=re_OCRed_text_filename\n",
    "Y=improvements\n",
    "sorted_re_OCRed_text_filename=[x for (y,x) in sorted(zip(Y,X))]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24986369527980024, 0.45334082086312893, 0.31080339096591891]\n",
      "['darts-226-398-A-de.pdf', 'darts-259-916-A-de.pdf', 'darts-935-617-C-de.pdf']\n",
      "['darts-226-398-A-de.pdf', 'darts-935-617-C-de.pdf', 'darts-259-916-A-de.pdf']\n",
      "['darts-935-617-C-de.pdf', 'darts-259-916-A-de.pdf']\n"
     ]
    }
   ],
   "source": [
    "print improvements\n",
    "print re_OCRed_text_filename\n",
    "print sorted_re_OCRed_text_filename\n",
    "print sorted_re_OCRed_text_filename[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_imp=0\n",
    "for index in range(len(re_OCRed_text_filename)):\n",
    "    if clf.predict_proba(X_array_test_scaled[test_text_filename.index(re_OCRed_text_filename[index])])[0][1]<clf.predict_proba(X_array_re_ocred_scaled[re_OCRed_text_filename.index(re_OCRed_text_filename[index])])[0][1] :\n",
    "        num_imp+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentange of documents which have been improved to very high quality\n",
      "0.666666666667\n",
      "\n",
      "Number of documents which have been improved to very high quality\n",
      "2\n",
      "\n",
      "Number of documents re-ocred\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Percentange of documents which have been improved to very high quality\"\n",
    "print len(X_array_re_ocred[X_array_re_ocred_pred==2])/len(re_OCRed_text_filename)\n",
    "print (\"\")\n",
    "\n",
    "print \"Number of documents which have been improved to very high quality\"\n",
    "print len(X_array_re_ocred[X_array_re_ocred_pred==2])\n",
    "print (\"\")\n",
    "\n",
    "print \"Number of documents re-ocred\"\n",
    "print len(re_OCRed_text_filename)\n",
    "print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of documents which has improved quality through re-ocring\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Percentage of documents which has improved quality through re-ocring\"\n",
    "print num_imp/len(re_OCRed_text_filename)\n",
    "print (\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re_ocr_backup_path=r'C:\\Users\\vbask\\Documents\\Darts_IP\\Darts_IP\\Factories\\German_Factory\\4_b_Re_OCRed_Backup'\n",
    "directory=str(datetime.now()).split(\" \")[0]+\"_\"+str(datetime.now()).split(\" \")[1].split(\":\")[0]+\"_\"+str(datetime.now()).split(\" \")[1].split(\":\")[1]\n",
    "os.makedirs(os.path.join(re_ocr_backup_path,directory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(re_OCRed_text_path):\n",
    "    shutil.copy(os.path.join(re_OCRed_text_path,f),os.path.join(re_ocr_backup_path,directory))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
