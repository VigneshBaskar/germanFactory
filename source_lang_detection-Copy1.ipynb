{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymysql\n",
    "import scipy.stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums=set()\n",
    "    else:\n",
    "        pagenums=set(pages)\n",
    "        \n",
    "    output=io.BytesIO()\n",
    "    manager=PDFResourceManager()\n",
    "    converter=TextConverter(manager, output, codec='utf-8', laparams=LAParams())\n",
    "    interpreter=PDFPageInterpreter(manager,converter)\n",
    "    \n",
    "    infile=file(fname,'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text=output.getvalue()\n",
    "    output.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "tbl = dict.fromkeys(i for i in xrange(sys.maxunicode)\n",
    "                      if unicodedata.category(unichr(i)).startswith('P'))\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def extract_quality_features(text,dictionary_words,show=1):\n",
    "    percent_tokens=[]\n",
    "    len_tokens=[]\n",
    "    text_processed=[]\n",
    "    for i in np.arange(len(text)):\n",
    "        temp=text[i].replace(u\"\\u2018\", \"\").replace(u\"\\u2019\", \"\").replace(u\"\\n\",\" \")\n",
    "        tokens=temp.split()\n",
    "        tokens=[remove_punctuation(word.lower()) for word in tokens]\n",
    "        punctuations = re.compile(r'[-./?!,--&\":;()|0-9]')\n",
    "        tokens=[punctuations.sub(\"\", word) for word in tokens if punctuations.sub(\"\", word)]\n",
    "        text_processed.append(\" \".join(tokens))\n",
    "        len_tokens.append(len(tokens))   \n",
    "\n",
    "    vect=CountVectorizer(vocabulary=dictionary_words)\n",
    "    X_counts=vect.transform(text_processed)\n",
    "    X_counts_sum=X_counts.sum(axis=1)\n",
    "    X_counts_sum = np.squeeze(np.asarray(X_counts_sum))\n",
    "\n",
    "    percent_tokens=[]\n",
    "    for i in range(len(text)):\n",
    "        temp=X_counts_sum[i]/len_tokens[i]\n",
    "        if np.isfinite(temp):\n",
    "            percent_tokens.append(temp if temp<1 else 1)\n",
    "        else:\n",
    "            percent_tokens.append(0)\n",
    "    if show:\n",
    "        print \"Do nothing\"\n",
    "    return percent_tokens,len_tokens,text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\vbask\\Documents\\Darts_IP\\Darts_IP\\Dictionaries')\n",
    "dictionary_words=pickle.load(open(\"latin_rus_dic.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darts-190-906-C-de.pdf', 'darts-226-398-A-de.pdf', 'darts-259-916-A-de.pdf', 'darts-510-738-B-de.pdf', 'darts-639-682-A-de.pdf', 'darts-935-617-C-de.pdf'] 6\n"
     ]
    }
   ],
   "source": [
    "gen_OCRed_text_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\2_b_Poor_OCRed\\\\Batch_prior\"\n",
    "os.chdir(gen_OCRed_text_path)\n",
    "\n",
    "gen_OCRed_text_filename=[f for f in os.listdir(gen_OCRed_text_path) if os.path.isfile(f) and os.path.splitext(f)[1]==\".pdf\"]\n",
    "print gen_OCRed_text_filename,\n",
    "print len(gen_OCRed_text_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success darts-190-906-C-de.pdf\n",
      "Success darts-226-398-A-de.pdf\n",
      "Success darts-259-916-A-de.pdf\n",
      "Success darts-510-738-B-de.pdf\n",
      "Success darts-639-682-A-de.pdf\n",
      "Success darts-935-617-C-de.pdf\n",
      "\n",
      "\n",
      "2.44000005722\n",
      "Non OCR Prediciton through error\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1=time.time()\n",
    "import os\n",
    "\n",
    "os.chdir(gen_OCRed_text_path)\n",
    "gen_OCRed_text=[]\n",
    "erroneous_fnames=[]\n",
    "non_erroneous_fnames=[]\n",
    "\n",
    "for fname in gen_OCRed_text_filename:\n",
    "    try:\n",
    "        print(\"Success \"+fname)\n",
    "        gen_OCRed_text.append(convert(fname).decode('utf-8','ignore'))\n",
    "        non_erroneous_fnames.append(fname)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        print('Some Error skipped with try')\n",
    "        print fname\n",
    "        erroneous_fnames.append(fname)\n",
    "\n",
    "time2=time.time()\n",
    "print('\\n')\n",
    "print (time2-time1)\n",
    "print('Non OCR Prediciton through error')\n",
    "print erroneous_fnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_OCRed_text_percent_tokens,gen_OCRed_text_len_tokens,gen_OCRed_text_processed=extract_quality_features(gen_OCRed_text,dictionary_words,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array_test=pickle.load(open(\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\X_array_test.p\",\"rb\"))\n",
    "test_text_filename=pickle.load(open(\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\test_text_filename.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poor_downloaded_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\1_Poor_downloaded\\\\Batch_prior\"\n",
    "poor_native_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\3_a_Poor_Native\"\n",
    "poor_img_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\2_a_Poor_img_layer\\\\Batch_prior\"\n",
    "poor_scanned_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\3_b_Poor_Scanned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_OCRed_text_filename=non_erroneous_fnames\n",
    "import shutil\n",
    "for root, dirs, files in os.walk(poor_native_path):\n",
    "    for f in files:\n",
    "        os.unlink(os.path.join(root, f))\n",
    "for root, dirs, files in os.walk(poor_scanned_path):\n",
    "    for f in files:\n",
    "        os.unlink(os.path.join(root, f))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Native_filename=[];Scanned_filename=[]\n",
    "for index in range(len(gen_OCRed_text_filename)):\n",
    "    if gen_OCRed_text_len_tokens[index] < 0.5*X_array_test[test_text_filename.index(gen_OCRed_text_filename[index])][1]:\n",
    "        Native_filename.append(gen_OCRed_text_filename[index])\n",
    "        shutil.copy(poor_downloaded_path+\"\\\\\"+gen_OCRed_text_filename[index],poor_native_path)\n",
    "    else:\n",
    "        Scanned_filename.append(gen_OCRed_text_filename[index])\n",
    "        shutil.copy(poor_img_path+\"\\\\\"+gen_OCRed_text_filename[index],poor_scanned_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darts-226-398-A-de.pdf\n",
      "name: German      code: de       confidence:  99.0 read bytes:  1018\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n",
      "\n",
      "\n",
      "darts-259-916-A-de.pdf\n",
      "name: German      code: de       confidence:  88.0 read bytes:   977\n",
      "name: English     code: en       confidence:  11.0 read bytes:   440\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n",
      "\n",
      "\n",
      "darts-935-617-C-de.pdf\n",
      "name: German      code: de       confidence:  97.0 read bytes:   919\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import polyglot\n",
    "from polyglot.detect import Detector\n",
    "for index in range(len(Scanned_filename)):\n",
    "    print(Scanned_filename[index])\n",
    "    try:\n",
    "        for language in Detector(gen_OCRed_text[gen_OCRed_text_filename.index(Scanned_filename[index])],quiet=True).languages:\n",
    "            print(language)\n",
    "    except:\n",
    "        print \"Do nothing\"\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ocr_languages=[\"en\",\"de\",\"fr\",\"es\",\"it\",\"ru\",\"nl\",\"pt\",\"sv\",\"tr\",\"th\",\"zh\",\"ja\",\"pl\",\"ko\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darts-226-398-A-de.pdf\n",
      "darts-259-916-A-de.pdf\n",
      "darts-935-617-C-de.pdf\n"
     ]
    }
   ],
   "source": [
    "detected_languages=[]\n",
    "for index in range(len(Scanned_filename)):\n",
    "    Confidence=[]\n",
    "    temp_detected_languages=[]\n",
    "    for language in Detector(gen_OCRed_text[gen_OCRed_text_filename.index(Scanned_filename[index])],quiet=True).languages:\n",
    "        try:\n",
    "            print gen_OCRed_text_filename[gen_OCRed_text_filename.index(Scanned_filename[index])]\n",
    "            if language.code in ocr_languages:\n",
    "                Confidence.append(language.confidence)\n",
    "                temp_detected_languages.append(language.code)\n",
    "            if sum(Confidence)>85:\n",
    "                break\n",
    "        except:\n",
    "            print \"Do nothing\"\n",
    "    if list(set(temp_detected_languages[:2])-set(['en','zh','ru']))==[]:\n",
    "        detected_languages.append([])\n",
    "    else:\n",
    "        detected_languages.append(temp_detected_languages[:2])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darts-226-398-A-de.pdf ['de']\n",
      "darts-259-916-A-de.pdf ['de']\n",
      "darts-935-617-C-de.pdf ['de']\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(Scanned_filename)):\n",
    "    print Scanned_filename[index],detected_languages[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations=[]\n",
    "for lang_1 in ocr_languages:\n",
    "    for lang_2 in ocr_languages:\n",
    "        if lang_1!=lang_2:\n",
    "            combinations.append(lang_1+\"_\"+lang_2)\n",
    "    ocr_languages=ocr_languages[1:]\n",
    "ocr_languages=[\"en\",\"de\",\"fr\",\"es\",\"it\",\"ru\",\"nl\",\"pt\",\"sv\",\"tr\",\"th\",\"zh\",\"ja\",\"pl\",\"ko\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for folder in combinations+ocr_languages:\n",
    "    directory=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\4_a_Re_OCRing_input\\\\\"+folder\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        \n",
    "for folder in combinations+ocr_languages:\n",
    "    directory=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\4_a_Re_OCRing_input\\\\\"+folder\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re_ocr_in_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\4_a_Re_OCRing_input\"\n",
    "re_ocr_out_path=\"C:\\\\Users\\\\vbask\\\\Documents\\\\Darts_IP\\\\Darts_IP\\\\Factories\\\\German_Factory\\\\4_b_Re_OCRed\"\n",
    "\n",
    "for root, dirs, files in os.walk(re_ocr_out_path):\n",
    "    for f in files:\n",
    "        try:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        except:\n",
    "            print\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in range(len(Scanned_filename)):\n",
    "    index_len=len(detected_languages[index])\n",
    "    if index_len==2:\n",
    "        if os.path.exists(re_ocr_in_path+\"\\\\\"+detected_languages[index][0]+\"_\"+detected_languages[index][1]):\n",
    "            shutil.copy(poor_scanned_path+\"\\\\\"+Scanned_filename[index],re_ocr_in_path+\"\\\\\"+detected_languages[index][0]+\"_\"+detected_languages[index][1])\n",
    "        elif os.path.exists(re_ocr_in_path+\"\\\\\"+detected_languages[index][1]+\"_\"+detected_languages[index][0]):\n",
    "            shutil.copy(poor_scanned_path+\"\\\\\"+Scanned_filename[index],re_ocr_in_path+\"\\\\\"+detected_languages[index][1]+\"_\"+detected_languages[index][0])\n",
    "    if index_len==1:\n",
    "        if os.path.exists(re_ocr_in_path+\"\\\\\"+detected_languages[index][0]):\n",
    "            shutil.copy(poor_scanned_path+\"\\\\\"+Scanned_filename[index],re_ocr_in_path+\"\\\\\"+detected_languages[index][0])\n",
    "    if index_len==0:\n",
    "        shutil.copy(poor_scanned_path+\"\\\\\"+Scanned_filename[index],re_ocr_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
